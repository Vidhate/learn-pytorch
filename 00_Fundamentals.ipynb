{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae82328",
   "metadata": {},
   "source": [
    "# 00 PyTorch Fundamentals\n",
    "Link to lesson - https://www.learnpytorch.io/00_pytorch_fundamentals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285b5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8068b2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa957bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b329f14f",
   "metadata": {},
   "source": [
    "## Important Learnings\n",
    "1. pt tensor creations are quite similar to numpy arrays  \n",
    "2. The shape, dtype and device are important properties of any pt tensor. Any mismatches might cause incompatibilities and are a significant cause of errors when using pt  \n",
    "3. Tensors don't change in-place  \n",
    "4. add, mul, div act elemetwise. When operating tensor with tensor, shape of smaller dim tensor should be contained in the larger dim tensor  \n",
    "5. Matrix multiplication is the basis of everything in DL. <= 2D Tensor mat-mul are as usual. For tensors with larger dimensions, the mat-mul is defined with rules detailed here : https://pytorch.org/docs/stable/generated/torch.matmul.html   \n",
    "    * Batched matrix multiplication is when two tensors of any number of dimension are treated as batches of matrices. Meaning tensor shaped (i,j,k,m,n) is treated as matrices of shape (m,n) in batches of (i,j,k). For batched matmul, the matrix dimensions should be consistent and the batches dimensions must be broadcastable.   \n",
    "    * Broadcastability documentation is https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics  \n",
    "    * Summary for broadcasting is - when iterating from last dimension to first for non-zero dimensional tensor, 2 shapes are broadcastable if -   \n",
    "        * either value of both dimensions matches exactly  \n",
    "        * one of the tensors has a value 1 for the corresponding dimension  \n",
    "        * one of the tensors has missing dimension  \n",
    "6. Aggregations without specifying dimensions will operate on the entire flattened tensor  \n",
    "7. Reshaping is possible with compatibility and -1 can be used to infer a single dim  \n",
    "    * Reshaping can return a copy or a view. Views share the same data, copy creates a new tensor  \n",
    "8. Important to think of any dimensional tensor as stored in a single contiguous list in memory, reshaping just decides how to group the list together in a desired way. Similarly any shape manipulation becomes easy to intuitively understand.  \n",
    "    * Total number of elements is a product of the shape  \n",
    "    * The dim value of 1 is useless and hence can be squeezed or unsqueezed without any issues   \n",
    "    * Permuting a tensor is different from reshaping as the order of data will alter, but permute only returns a view   \n",
    "9. Slicing and dicing of tensors works similar to python lists, indexing can be comma separated or [ ] separated   \n",
    "10. Numpy default arrays are float64 whereas PyTorch are float32, important note to keep in mind when converting between numpy to pt and reverse  \n",
    "11. Reproducibility is an extensive topic in ML experiments and needs to be explicitly considered for due to the use of various RNG (Random Number Generators) throughout pt, python, numpy and GPU devices too. For reproducible results a lot could have to be done, beginning at setting the random seed.  \n",
    "12. Torch interfaces with GPUs (NVIDIA, M2) using API's developed by the hardware owners to expose control of low level system. For NVIDIA CUDA is the API, for Apple Silicon it's MPS. \n",
    "    * Data copies will be created when sent to the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2c302",
   "metadata": {},
   "source": [
    "Many ops are not yet supported on the MPS backend. Development to add support is active on the pytorch page. Contributions can be made here : https://github.com/pytorch/pytorch/wiki/MPS-Backend#adding-op-for-mps-backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50deb306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41f5ed30",
   "metadata": {},
   "source": [
    "### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc3ff8",
   "metadata": {},
   "source": [
    "__ndim__ _(property)_ is used to check the dimensions of a pytorch tensor  \n",
    "__item__ _(class function)_ to return the element stored in tensor (only works with scalars)  \n",
    "__shape__ _(property)_ returns how the tensor is organized  \n",
    "0 dimensional tensor is a scalar. 1 dimensional tensor is a vector.  \n",
    "  \n",
    "__rand( )__ _(function)_ creates a tensor of given _size_ populated with random numbers  \n",
    "__zeros( )__/__ones( )__ _(function)_ creates a tensor of given _size_ with just 0 and 1 populated  \n",
    "__zeros_like( )__/__ones_like( )__ _(function)_ creates a tensor of a similar shape as that of tensor given to this function  \n",
    "__arange( )__ _(function)_ same usage as numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d836a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(7.0)\n",
    "vector = torch.tensor([5,2])\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "365b3ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(7.),\n",
       " tensor([5, 2]),\n",
       " tensor([[[1, 2, 3],\n",
       "          [3, 6, 9],\n",
       "          [2, 4, 5]]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar, vector, TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5016cbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim, vector.ndim, TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7827d21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.0, float)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item(), type(scalar.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed3f95fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape, TENSOR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08ebc1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0360, 0.3543, 0.0354, 0.3270, 0.2285],\n",
       "         [0.1905, 0.0526, 0.2254, 0.6990, 0.8712],\n",
       "         [0.3416, 0.5381, 0.3956, 0.9644, 0.2166],\n",
       "         [0.5363, 0.3536, 0.4677, 0.1796, 0.8533]],\n",
       "\n",
       "        [[0.5685, 0.5034, 0.2518, 0.1161, 0.4023],\n",
       "         [0.6697, 0.5040, 0.4035, 0.0095, 0.1197],\n",
       "         [0.7293, 0.5402, 0.3124, 0.6993, 0.6913],\n",
       "         [0.6106, 0.4533, 0.6542, 0.5759, 0.4670]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(size=(2,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a8d26ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(size=(2,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23ad2177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(size=(2,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f11ab21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0000, 10.0800, 10.1600, 10.2400, 10.3200, 10.4000, 10.4800])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start=10, end=10.5, step=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df41da11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.arange(start=10, end=10.5, step=0.08))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "01fe0f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(size=(2,2))\n",
    "t.shape, t.dtype, t.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3213aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d571806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b11f0a",
   "metadata": {},
   "source": [
    "### Operating on Tensors\n",
    "Addition, subtraction, multiplication, division, matrix multiplication on tensor - scalar, tensor - tensor  \n",
    "1. +, .add(), \\*, .mul(), .multiply(), /, .div(), .divide(),  all act element-wise on given scalar to tensor  \n",
    "2. all functions are element wise when operating 2 tensors, implying one tensor shape must be contained in another tensor shape (else error) and operations will be done element wise for each matching tensor dim   \n",
    "3. .matmul (property, function) is used for matrix multiplication of 2 tensors. Details on rules used for matmuls are given above in important learnings  \n",
    "    * matmul is much faster than doing matrix multiplications manually (why?)  \n",
    "    * shape issues when doing matmul are the root of most causes in DL  \n",
    "4. __transporse( )__/__.T__ _(function)/(property)_ to transpose a tensor along desired dimensions or a tensor of dim 2 directly using the property .T  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64c98a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9928, 0.9058],\n",
       "        [0.3638, 0.4534]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0c9982b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7689, 0.7921],\n",
       "         [0.6978, 0.5697]],\n",
       "\n",
       "        [[0.6212, 0.8232],\n",
       "         [0.2646, 0.8914]],\n",
       "\n",
       "        [[0.2974, 0.9238],\n",
       "         [0.3670, 0.4753]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.rand(size=(3,2,2))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "711cd20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([2, 4, 2, 2]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d1cc510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.9928, 10.9058],\n",
       "         [10.3638, 10.4534]]),\n",
       " tensor([[10.9928, 10.9058],\n",
       "         [10.3638, 10.4534]]),\n",
       " tensor([[10.9928, 10.9058],\n",
       "         [10.3638, 10.4534]]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar addition\n",
    "t+10, t.add(10), torch.add(t, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd3a5cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2239,  0.1137],\n",
       "         [-0.3340, -0.1163]],\n",
       "\n",
       "        [[ 0.3716,  0.0826],\n",
       "         [ 0.0992, -0.4380]],\n",
       "\n",
       "        [[ 0.6954, -0.0180],\n",
       "         [-0.0032, -0.0219]]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor addition\n",
    "t+m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8929961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True]],\n",
       "\n",
       "        [[True, True],\n",
       "         [True, True]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m-t == (t-m)*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e7ff9f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.9277, 9.0582],\n",
       "         [3.6383, 4.5340]]),\n",
       " tensor([[9.9277, 9.0582],\n",
       "         [3.6383, 4.5340]]),\n",
       " tensor([[9.9277, 9.0582],\n",
       "         [3.6383, 4.5340]]),\n",
       " tensor([[9.9277, 9.0582],\n",
       "         [3.6383, 4.5340]]),\n",
       " tensor([[9.9277, 9.0582],\n",
       "         [3.6383, 4.5340]]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t*10, t.mul(10), torch.mul(t,10), t.multiply(10), torch.multiply(t,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42109ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7633, 0.7175],\n",
       "         [0.2539, 0.2583]],\n",
       "\n",
       "        [[0.6167, 0.7457],\n",
       "         [0.0963, 0.4042]],\n",
       "\n",
       "        [[0.2952, 0.8368],\n",
       "         [0.1335, 0.2155]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f450b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0993, 0.0906],\n",
       "         [0.0364, 0.0453]]),\n",
       " tensor([[0.0993, 0.0906],\n",
       "         [0.0364, 0.0453]]),\n",
       " tensor([[0.0993, 0.0906],\n",
       "         [0.0364, 0.0453]]),\n",
       " tensor([[0.0993, 0.0906],\n",
       "         [0.0364, 0.0453]]),\n",
       " tensor([[0.0993, 0.0906],\n",
       "         [0.0364, 0.0453]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t/10, t.div(10), torch.divide(t,10), t.divide(10), torch.divide(t,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "43e6dc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7745, 0.8745],\n",
       "         [1.9180, 1.2566]],\n",
       "\n",
       "        [[0.6257, 0.9088],\n",
       "         [0.7273, 1.9661]],\n",
       "\n",
       "        [[0.2996, 1.0199],\n",
       "         [1.0087, 1.0484]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00a3217f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m/t).shape == (t/m).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1507f1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([3, 2, 2]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f17306a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(5,4,3,4,7))\n",
    "y = torch.rand(size=(4,1,7,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae340e85",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[2.4363, 1.5432, 2.6072,  ..., 2.4733, 1.6972, 2.2607],\n",
       "           [2.4275, 1.5164, 2.5356,  ..., 2.1790, 1.9430, 2.0773],\n",
       "           [2.0940, 1.0981, 1.7974,  ..., 1.7294, 1.4222, 1.8638],\n",
       "           [1.8491, 0.9817, 1.4798,  ..., 1.6236, 1.2287, 1.8035]],\n",
       "\n",
       "          [[2.0091, 1.0551, 1.7251,  ..., 1.5210, 1.3595, 1.8177],\n",
       "           [1.8589, 1.2450, 2.1787,  ..., 2.1714, 1.4107, 1.5744],\n",
       "           [2.1413, 0.8685, 1.7088,  ..., 1.9529, 1.6217, 1.7080],\n",
       "           [1.1204, 0.8282, 1.3416,  ..., 1.5589, 0.9096, 1.0723]],\n",
       "\n",
       "          [[2.1966, 1.1779, 1.9153,  ..., 2.5672, 1.5888, 1.9177],\n",
       "           [1.4870, 0.9037, 1.5308,  ..., 1.5523, 1.2204, 1.2740],\n",
       "           [1.8338, 0.8221, 1.7083,  ..., 1.5298, 1.3873, 1.4073],\n",
       "           [2.0110, 1.0445, 1.8971,  ..., 1.9886, 1.4120, 1.6990]]],\n",
       "\n",
       "\n",
       "         [[[2.1205, 1.9821, 1.7572,  ..., 1.6772, 0.7730, 1.5832],\n",
       "           [2.2539, 2.0237, 1.5418,  ..., 1.7148, 0.9611, 1.4629],\n",
       "           [3.0011, 2.5646, 1.8128,  ..., 2.0962, 1.7195, 2.3619],\n",
       "           [1.0981, 1.0818, 0.8418,  ..., 0.9110, 0.5995, 0.8945]],\n",
       "\n",
       "          [[2.5300, 2.3579, 1.6805,  ..., 1.9484, 1.4036, 1.5296],\n",
       "           [3.0733, 2.9883, 2.0899,  ..., 2.3138, 1.5531, 2.1501],\n",
       "           [1.9273, 1.8513, 1.9039,  ..., 1.6286, 0.8381, 1.4039],\n",
       "           [2.5237, 2.4220, 1.5688,  ..., 1.9728, 0.9310, 1.4940]],\n",
       "\n",
       "          [[1.5839, 1.7238, 0.7861,  ..., 1.3206, 0.8966, 1.0620],\n",
       "           [3.7279, 3.7405, 2.7699,  ..., 2.9923, 1.9367, 2.9374],\n",
       "           [2.2081, 1.6824, 1.4775,  ..., 1.3970, 1.3062, 1.6213],\n",
       "           [2.6236, 2.5446, 2.1717,  ..., 2.0598, 1.0774, 1.6555]]],\n",
       "\n",
       "\n",
       "         [[[1.1557, 1.6116, 2.2853,  ..., 2.0850, 1.2310, 1.4719],\n",
       "           [1.7047, 1.9994, 2.4969,  ..., 2.6358, 1.7862, 1.7428],\n",
       "           [1.3744, 2.0663, 2.4213,  ..., 2.1104, 1.6573, 1.9977],\n",
       "           [1.3463, 1.8481, 2.0013,  ..., 2.0644, 1.6510, 1.7678]],\n",
       "\n",
       "          [[0.8281, 1.0605, 0.9774,  ..., 1.1414, 1.0525, 0.9824],\n",
       "           [1.3711, 1.4919, 2.0564,  ..., 2.0626, 1.3706, 1.1481],\n",
       "           [1.7228, 2.1011, 2.6946,  ..., 2.7712, 1.9690, 1.7024],\n",
       "           [0.9878, 1.2635, 1.4942,  ..., 1.4950, 0.9119, 1.3491]],\n",
       "\n",
       "          [[1.6573, 1.8538, 2.3686,  ..., 2.5192, 1.4918, 1.7897],\n",
       "           [1.0750, 1.6049, 1.8923,  ..., 1.6577, 1.3170, 1.5219],\n",
       "           [1.3909, 1.9749, 2.6000,  ..., 2.3000, 1.7992, 1.5702],\n",
       "           [1.7114, 2.3301, 3.2525,  ..., 2.7205, 1.7830, 2.1038]]],\n",
       "\n",
       "\n",
       "         [[[1.5565, 1.3820, 2.8710,  ..., 2.6596, 2.8019, 1.9078],\n",
       "           [2.8305, 1.6815, 2.4877,  ..., 2.4183, 2.5088, 2.6434],\n",
       "           [1.9951, 1.3594, 2.4378,  ..., 1.9704, 2.2946, 2.3581],\n",
       "           [1.5381, 1.0588, 1.9132,  ..., 1.4400, 1.7834, 1.7762]],\n",
       "\n",
       "          [[2.3155, 1.9433, 2.9787,  ..., 2.6431, 2.7214, 2.4459],\n",
       "           [2.3140, 1.8967, 3.1406,  ..., 2.6324, 2.8286, 2.0635],\n",
       "           [1.6467, 1.6849, 1.8120,  ..., 1.7276, 1.3118, 1.3815],\n",
       "           [1.3909, 1.2059, 1.6887,  ..., 0.8648, 1.3968, 1.2223]],\n",
       "\n",
       "          [[1.4664, 1.4869, 1.7092,  ..., 1.5849, 1.3406, 1.4545],\n",
       "           [1.2946, 0.9271, 1.6491,  ..., 1.1899, 1.4869, 1.1028],\n",
       "           [2.1303, 1.7525, 1.9156,  ..., 1.4158, 1.3351, 1.3585],\n",
       "           [2.1766, 1.7358, 2.4833,  ..., 1.8580, 2.1231, 2.0428]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.7854, 1.1847, 1.9411,  ..., 1.9825, 1.2366, 1.6816],\n",
       "           [3.2404, 1.3593, 2.4729,  ..., 2.6751, 2.4586, 2.5221],\n",
       "           [2.7243, 1.6808, 2.5110,  ..., 2.4599, 1.9759, 2.6239],\n",
       "           [2.5148, 1.0365, 1.8726,  ..., 2.0087, 2.0338, 1.8784]],\n",
       "\n",
       "          [[1.9644, 1.4966, 2.4378,  ..., 2.1863, 1.4714, 1.8323],\n",
       "           [1.9712, 0.9477, 1.5473,  ..., 1.4651, 1.2936, 1.7704],\n",
       "           [3.2799, 1.9001, 3.2734,  ..., 3.1966, 2.6644, 2.7172],\n",
       "           [2.4550, 1.7479, 2.6516,  ..., 2.6366, 2.0240, 2.2653]],\n",
       "\n",
       "          [[2.4867, 1.3243, 2.3791,  ..., 2.2569, 1.8973, 1.9623],\n",
       "           [2.8584, 1.4732, 2.6823,  ..., 2.2684, 2.3431, 2.3074],\n",
       "           [2.2702, 1.2207, 1.8399,  ..., 2.0471, 1.5858, 2.0759],\n",
       "           [2.7614, 1.8371, 2.8236,  ..., 2.6010, 1.9214, 2.6972]]],\n",
       "\n",
       "\n",
       "         [[[3.0086, 3.0783, 1.8934,  ..., 2.3402, 1.5556, 1.8533],\n",
       "           [2.9038, 2.5877, 2.2286,  ..., 2.1496, 1.5825, 2.1703],\n",
       "           [3.0098, 2.7836, 2.2914,  ..., 2.2181, 1.7320, 2.3081],\n",
       "           [1.8485, 1.6471, 1.3154,  ..., 1.3461, 0.9359, 1.0530]],\n",
       "\n",
       "          [[2.2705, 1.8755, 1.3973,  ..., 1.5323, 1.2932, 1.4786],\n",
       "           [2.7860, 2.4794, 1.7325,  ..., 2.0229, 1.5080, 1.7480],\n",
       "           [1.8121, 1.5842, 1.1713,  ..., 1.2979, 0.7544, 1.0847],\n",
       "           [1.9414, 1.6924, 1.2265,  ..., 1.4278, 0.8574, 1.1085]],\n",
       "\n",
       "          [[2.9490, 2.6666, 2.5921,  ..., 2.2422, 1.4807, 2.1166],\n",
       "           [3.3119, 3.2035, 2.1519,  ..., 2.4898, 1.6957, 2.2228],\n",
       "           [2.6616, 2.2855, 2.0129,  ..., 1.8881, 1.4280, 2.0826],\n",
       "           [2.1971, 2.1543, 1.0719,  ..., 1.7115, 1.3665, 1.4948]]],\n",
       "\n",
       "\n",
       "         [[[1.6783, 2.6082, 3.2405,  ..., 3.1753, 2.0257, 2.6214],\n",
       "           [2.1882, 2.2060, 2.7627,  ..., 2.8340, 1.8448, 2.0435],\n",
       "           [1.4338, 2.0222, 2.3577,  ..., 2.1099, 1.6655, 1.9225],\n",
       "           [1.1450, 1.4715, 1.9993,  ..., 1.6542, 1.3069, 1.1587]],\n",
       "\n",
       "          [[1.3010, 1.5919, 2.0481,  ..., 1.6434, 1.1785, 1.5287],\n",
       "           [0.4418, 0.7032, 0.8557,  ..., 0.9404, 0.6200, 0.6607],\n",
       "           [1.0818, 1.8267, 2.3374,  ..., 2.2150, 1.5265, 1.6955],\n",
       "           [1.1534, 1.7383, 1.8845,  ..., 1.8451, 1.4607, 1.7310]],\n",
       "\n",
       "          [[1.5900, 2.1848, 2.5710,  ..., 2.3399, 2.0260, 1.8541],\n",
       "           [1.5007, 1.7878, 2.3910,  ..., 1.8508, 1.2646, 1.7169],\n",
       "           [1.6047, 2.4717, 3.1703,  ..., 2.9564, 1.9890, 2.3251],\n",
       "           [0.7755, 1.5658, 2.0064,  ..., 1.9996, 1.2459, 1.5590]]],\n",
       "\n",
       "\n",
       "         [[[2.1859, 1.9488, 2.2155,  ..., 1.8887, 1.8595, 1.2592],\n",
       "           [1.2547, 0.6844, 1.3428,  ..., 1.2372, 1.4435, 0.8843],\n",
       "           [1.4584, 0.8925, 1.8475,  ..., 1.2739, 1.8033, 1.3797],\n",
       "           [1.9127, 1.4819, 1.5748,  ..., 1.6259, 1.5029, 1.3597]],\n",
       "\n",
       "          [[1.7795, 1.4114, 1.5737,  ..., 1.1292, 1.1266, 1.0776],\n",
       "           [1.5633, 1.3783, 1.8035,  ..., 1.4397, 1.5905, 1.6533],\n",
       "           [1.6188, 1.7189, 1.6462,  ..., 1.7271, 1.4384, 1.6984],\n",
       "           [2.5063, 1.7350, 2.4034,  ..., 2.1247, 2.2629, 1.6419]],\n",
       "\n",
       "          [[1.7500, 1.4041, 1.7409,  ..., 1.2705, 1.3349, 0.6710],\n",
       "           [1.9703, 1.2501, 2.0258,  ..., 1.5026, 2.0462, 1.6583],\n",
       "           [2.4045, 2.2376, 2.4583,  ..., 2.0408, 1.8228, 1.9306],\n",
       "           [2.4215, 1.7814, 2.3123,  ..., 1.8804, 2.1867, 1.6399]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[2.0557, 0.9948, 1.8688,  ..., 1.4053, 1.4468, 1.7240],\n",
       "           [2.5805, 1.0684, 2.0182,  ..., 2.3861, 1.9678, 2.1198],\n",
       "           [2.2754, 0.9109, 1.4662,  ..., 1.8325, 1.7268, 1.9215],\n",
       "           [2.2337, 1.6105, 2.4058,  ..., 2.8262, 1.6210, 2.2296]],\n",
       "\n",
       "          [[3.2639, 1.7463, 3.0407,  ..., 2.9077, 2.6526, 2.5958],\n",
       "           [2.6315, 1.5542, 2.3342,  ..., 2.4977, 1.9604, 2.4676],\n",
       "           [1.7673, 1.2293, 1.9432,  ..., 1.3779, 1.3716, 1.6222],\n",
       "           [2.9414, 1.3588, 2.2975,  ..., 2.1329, 2.2861, 2.4006]],\n",
       "\n",
       "          [[1.3583, 0.5719, 1.0246,  ..., 1.1740, 0.8124, 1.2334],\n",
       "           [2.8766, 1.9180, 2.8410,  ..., 3.0174, 2.1763, 2.7714],\n",
       "           [1.6880, 0.6201, 1.1838,  ..., 1.1882, 1.3825, 1.2084],\n",
       "           [1.6305, 0.7432, 1.3255,  ..., 1.6467, 1.1945, 1.4356]]],\n",
       "\n",
       "\n",
       "         [[[3.2478, 2.8981, 2.0073,  ..., 2.3518, 1.8066, 2.3217],\n",
       "           [1.4707, 1.2914, 1.3080,  ..., 1.1935, 0.7420, 1.0840],\n",
       "           [2.0488, 1.9973, 1.3116,  ..., 1.6413, 0.8792, 1.2784],\n",
       "           [2.5468, 2.5882, 1.9016,  ..., 2.1621, 1.5002, 2.0219]],\n",
       "\n",
       "          [[2.6080, 2.1653, 1.8769,  ..., 1.7598, 1.2971, 1.9415],\n",
       "           [2.8335, 2.4148, 1.9444,  ..., 2.0129, 1.4612, 1.9533],\n",
       "           [2.6671, 2.6506, 2.2243,  ..., 2.1662, 1.1740, 1.6727],\n",
       "           [1.8300, 1.8453, 1.1244,  ..., 1.4657, 1.1411, 1.2980]],\n",
       "\n",
       "          [[2.5799, 2.5760, 1.8221,  ..., 2.1066, 1.6126, 1.9577],\n",
       "           [2.5282, 2.6763, 2.5301,  ..., 2.3011, 0.9417, 1.8961],\n",
       "           [2.5245, 2.4210, 1.8953,  ..., 1.8446, 1.3436, 1.9386],\n",
       "           [2.3383, 2.2591, 2.0682,  ..., 1.9473, 1.0387, 1.5347]]],\n",
       "\n",
       "\n",
       "         [[[1.0398, 1.4004, 1.8844,  ..., 1.7672, 1.2116, 1.1915],\n",
       "           [1.3348, 2.1478, 2.6292,  ..., 2.4321, 1.9506, 1.8421],\n",
       "           [1.2714, 1.6714, 1.9781,  ..., 1.8662, 1.3530, 1.6181],\n",
       "           [1.6800, 2.2393, 2.7464,  ..., 2.3157, 2.0172, 1.8615]],\n",
       "\n",
       "          [[1.2836, 1.8205, 2.5604,  ..., 1.9601, 1.4587, 1.5152],\n",
       "           [1.4967, 2.3926, 3.0718,  ..., 2.7652, 1.9364, 2.2243],\n",
       "           [1.9530, 2.4708, 3.3848,  ..., 2.9353, 2.0701, 2.0600],\n",
       "           [0.4880, 1.0792, 1.4171,  ..., 1.1002, 0.9765, 0.8379]],\n",
       "\n",
       "          [[1.4590, 1.5581, 1.7105,  ..., 1.6160, 1.5624, 1.2188],\n",
       "           [1.5139, 2.1283, 2.4442,  ..., 2.4233, 1.7216, 2.1223],\n",
       "           [1.1986, 2.0622, 2.8782,  ..., 2.5457, 1.5613, 1.9255],\n",
       "           [1.2550, 2.2708, 2.9785,  ..., 2.6410, 1.8228, 2.1098]]],\n",
       "\n",
       "\n",
       "         [[[3.1153, 2.2773, 3.0047,  ..., 2.4704, 2.4731, 2.3692],\n",
       "           [2.5129, 1.8248, 2.5885,  ..., 2.3231, 2.2560, 1.4904],\n",
       "           [2.2626, 1.9753, 2.2683,  ..., 2.0721, 1.6936, 1.9693],\n",
       "           [1.1051, 1.3441, 1.5928,  ..., 1.6939, 1.1710, 1.1338]],\n",
       "\n",
       "          [[1.0702, 1.3264, 1.5798,  ..., 1.5967, 1.4238, 1.0602],\n",
       "           [2.2829, 1.6956, 3.0458,  ..., 2.4591, 2.7358, 1.8891],\n",
       "           [1.1158, 1.2331, 1.7423,  ..., 1.5813, 1.5249, 1.1329],\n",
       "           [1.9609, 1.7090, 2.0267,  ..., 2.2091, 1.8581, 2.0470]],\n",
       "\n",
       "          [[1.9566, 1.3570, 2.2207,  ..., 1.9689, 2.1063, 1.9543],\n",
       "           [2.2215, 1.9376, 1.9728,  ..., 2.1952, 1.7386, 1.8712],\n",
       "           [1.2266, 1.2240, 1.0883,  ..., 1.3192, 1.1172, 1.0829],\n",
       "           [3.2827, 2.4635, 3.4994,  ..., 2.9228, 3.1246, 2.6258]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[2.2352, 0.9021, 1.7293,  ..., 1.6195, 1.5653, 1.8675],\n",
       "           [2.1315, 1.3822, 1.9920,  ..., 2.0756, 1.7292, 1.9350],\n",
       "           [2.6595, 0.9671, 1.9933,  ..., 1.9551, 2.0781, 1.9008],\n",
       "           [2.1721, 1.2871, 2.4430,  ..., 2.1278, 1.9227, 1.6455]],\n",
       "\n",
       "          [[3.0975, 1.3854, 2.4366,  ..., 2.0413, 2.3971, 2.5364],\n",
       "           [2.4829, 0.9485, 1.9785,  ..., 1.4913, 1.7519, 1.9111],\n",
       "           [0.8615, 0.7316, 1.2076,  ..., 1.1183, 0.6590, 0.8760],\n",
       "           [2.1261, 1.7456, 2.7308,  ..., 3.1000, 1.5622, 2.1407]],\n",
       "\n",
       "          [[2.2423, 1.4355, 2.3986,  ..., 2.7692, 1.6310, 2.1021],\n",
       "           [2.2962, 1.1885, 1.8139,  ..., 2.2212, 1.7339, 2.1427],\n",
       "           [2.4487, 1.6362, 2.5961,  ..., 2.0776, 1.7553, 2.3477],\n",
       "           [3.1692, 1.4331, 2.7336,  ..., 2.6374, 2.5167, 2.4512]]],\n",
       "\n",
       "\n",
       "         [[[2.0450, 1.8725, 0.8394,  ..., 1.3698, 1.3058, 1.4242],\n",
       "           [3.3720, 3.2224, 2.9157,  ..., 2.7279, 1.4683, 2.2337],\n",
       "           [1.4002, 1.4235, 1.1548,  ..., 1.1295, 0.8112, 1.1371],\n",
       "           [2.5962, 2.4215, 1.7077,  ..., 1.9334, 1.5747, 1.8651]],\n",
       "\n",
       "          [[2.3031, 2.0873, 1.7971,  ..., 1.7467, 1.3313, 1.7085],\n",
       "           [1.2597, 1.1603, 0.7753,  ..., 0.9601, 0.7405, 0.8323],\n",
       "           [3.1630, 3.1653, 2.0619,  ..., 2.5489, 1.4771, 2.1919],\n",
       "           [1.6076, 1.6635, 0.7924,  ..., 1.2845, 1.0034, 1.2166]],\n",
       "\n",
       "          [[3.1568, 3.1523, 2.0135,  ..., 2.4650, 1.5194, 2.1260],\n",
       "           [2.3937, 2.1592, 1.7072,  ..., 1.7063, 1.2168, 1.8412],\n",
       "           [1.7029, 1.6431, 0.8930,  ..., 1.2493, 0.7886, 1.1301],\n",
       "           [2.0134, 2.0485, 1.7843,  ..., 1.7511, 0.6325, 1.3264]]],\n",
       "\n",
       "\n",
       "         [[[0.7227, 1.3099, 1.9558,  ..., 1.5382, 1.2036, 0.9093],\n",
       "           [1.4323, 2.8267, 3.5077,  ..., 3.0938, 2.2583, 2.7364],\n",
       "           [1.7642, 2.1919, 2.8718,  ..., 2.9628, 1.9544, 1.8770],\n",
       "           [1.2940, 1.3293, 1.4384,  ..., 1.3212, 1.1447, 1.2464]],\n",
       "\n",
       "          [[1.2655, 1.4905, 1.9997,  ..., 1.9467, 1.2944, 1.2645],\n",
       "           [0.9753, 1.3867, 2.1539,  ..., 1.8368, 1.1319, 1.0810],\n",
       "           [1.8367, 2.7144, 3.4328,  ..., 3.3423, 2.3227, 2.4530],\n",
       "           [0.9445, 1.7049, 2.0554,  ..., 1.9140, 1.5730, 1.4588]],\n",
       "\n",
       "          [[1.8513, 2.3684, 3.2595,  ..., 2.8467, 1.8382, 2.1377],\n",
       "           [1.8098, 2.0922, 2.4398,  ..., 2.5663, 2.0533, 1.6899],\n",
       "           [1.2626, 1.7244, 2.4729,  ..., 2.1710, 1.3755, 1.4582],\n",
       "           [1.6521, 1.8163, 2.0197,  ..., 2.0766, 1.5266, 1.7853]]],\n",
       "\n",
       "\n",
       "         [[[2.3237, 1.8451, 2.5674,  ..., 2.1114, 2.1792, 1.2995],\n",
       "           [2.3921, 1.9015, 2.4198,  ..., 2.4170, 2.1940, 1.7408],\n",
       "           [1.9624, 1.3490, 1.9095,  ..., 1.7782, 1.7083, 1.7029],\n",
       "           [1.7897, 1.5950, 2.1319,  ..., 2.0467, 1.9128, 2.0316]],\n",
       "\n",
       "          [[2.7541, 1.9909, 3.0224,  ..., 2.3414, 2.7074, 2.1940],\n",
       "           [2.9370, 2.2124, 3.0244,  ..., 2.6709, 2.7229, 2.3787],\n",
       "           [2.2362, 1.2208, 2.3533,  ..., 1.8446, 2.2156, 1.4872],\n",
       "           [1.2363, 1.3151, 2.0177,  ..., 1.6868, 1.8543, 1.2144]],\n",
       "\n",
       "          [[1.7584, 1.3129, 1.3660,  ..., 1.2755, 1.1147, 0.9501],\n",
       "           [2.8422, 1.8495, 2.4760,  ..., 1.9933, 2.1063, 1.8419],\n",
       "           [2.1125, 1.6038, 2.1198,  ..., 1.4926, 1.8856, 1.1718],\n",
       "           [1.1035, 1.3693, 2.3059,  ..., 2.0076, 2.0654, 1.1935]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[1.7913, 1.0594, 1.6155,  ..., 1.1857, 1.3254, 1.5765],\n",
       "           [1.8933, 1.1492, 1.9461,  ..., 1.9884, 1.4723, 1.5700],\n",
       "           [2.8203, 1.2752, 2.0787,  ..., 2.0393, 2.1205, 2.4475],\n",
       "           [2.9827, 1.3072, 2.2069,  ..., 2.5719, 2.2679, 2.4003]],\n",
       "\n",
       "          [[2.0830, 0.7172, 1.5208,  ..., 1.2849, 1.7472, 1.4271],\n",
       "           [2.5892, 1.4967, 2.6616,  ..., 2.5963, 2.0642, 2.1264],\n",
       "           [2.1438, 1.0823, 1.8624,  ..., 2.1601, 1.5883, 1.9051],\n",
       "           [1.9963, 0.7073, 1.5320,  ..., 1.1508, 1.6212, 1.3897]],\n",
       "\n",
       "          [[1.4192, 0.5931, 1.2700,  ..., 0.7565, 1.0840, 1.0042],\n",
       "           [1.5941, 1.0658, 1.8037,  ..., 1.5637, 1.1833, 1.4165],\n",
       "           [2.4145, 1.7339, 2.7290,  ..., 3.0440, 1.9008, 2.2382],\n",
       "           [2.0472, 1.1743, 1.8266,  ..., 1.9859, 1.6208, 1.8424]]],\n",
       "\n",
       "\n",
       "         [[[3.5557, 3.5081, 2.6967,  ..., 2.8375, 1.9244, 2.7471],\n",
       "           [1.1054, 1.1894, 0.7858,  ..., 0.9125, 0.6074, 0.7204],\n",
       "           [2.5436, 2.3812, 1.1421,  ..., 1.8165, 1.5007, 1.9362],\n",
       "           [3.1256, 2.9251, 1.8381,  ..., 2.2878, 1.8623, 2.4420]],\n",
       "\n",
       "          [[2.0681, 1.8859, 1.3132,  ..., 1.5664, 1.4062, 1.6348],\n",
       "           [2.0158, 1.7924, 0.8227,  ..., 1.3602, 1.0070, 1.2939],\n",
       "           [2.4047, 2.3090, 1.9725,  ..., 1.9271, 1.3100, 1.8662],\n",
       "           [2.2880, 2.2466, 1.6590,  ..., 1.7894, 1.4549, 2.0150]],\n",
       "\n",
       "          [[3.1056, 3.1094, 1.5585,  ..., 2.3671, 1.7801, 2.1149],\n",
       "           [3.1506, 3.1604, 2.1445,  ..., 2.4732, 1.4941, 1.9687],\n",
       "           [3.2421, 2.8607, 1.7785,  ..., 2.2809, 2.0051, 2.3895],\n",
       "           [2.3031, 2.4251, 1.1559,  ..., 1.8542, 1.2589, 1.3998]]],\n",
       "\n",
       "\n",
       "         [[[1.4205, 1.9851, 2.2572,  ..., 2.1903, 1.8006, 1.7659],\n",
       "           [1.6809, 1.8672, 2.2007,  ..., 2.1123, 1.4862, 1.8042],\n",
       "           [0.6891, 1.3484, 1.9338,  ..., 1.7733, 0.9542, 1.3516],\n",
       "           [1.5392, 2.1479, 2.5718,  ..., 2.5743, 1.8910, 1.9515]],\n",
       "\n",
       "          [[1.8314, 1.7364, 2.4240,  ..., 2.2581, 1.5312, 1.3551],\n",
       "           [1.7482, 1.8522, 2.4697,  ..., 2.2620, 1.6446, 1.4816],\n",
       "           [1.8081, 2.4580, 3.5764,  ..., 2.9697, 1.9603, 2.0487],\n",
       "           [1.3744, 1.6874, 2.3273,  ..., 2.1774, 1.5325, 1.3126]],\n",
       "\n",
       "          [[1.2585, 1.5675, 1.9452,  ..., 2.1762, 1.2109, 1.6124],\n",
       "           [1.5247, 2.2790, 2.7366,  ..., 2.5497, 2.0947, 1.9530],\n",
       "           [0.8358, 1.5278, 1.8586,  ..., 1.5500, 1.2822, 1.3966],\n",
       "           [1.5505, 1.3634, 1.6982,  ..., 1.5602, 1.2207, 1.1045]]],\n",
       "\n",
       "\n",
       "         [[[2.1342, 1.6667, 2.6357,  ..., 2.4003, 2.4005, 2.0525],\n",
       "           [2.2910, 2.1076, 2.5652,  ..., 2.0584, 2.0002, 1.6924],\n",
       "           [2.4821, 1.7512, 2.5242,  ..., 2.4408, 2.5858, 2.4590],\n",
       "           [1.8358, 1.4442, 2.6938,  ..., 2.3380, 2.5955, 1.5658]],\n",
       "\n",
       "          [[2.4280, 1.7197, 2.2455,  ..., 1.9745, 2.1767, 1.8819],\n",
       "           [1.0884, 1.4017, 2.1162,  ..., 1.7434, 1.7400, 1.0117],\n",
       "           [1.0943, 0.9620, 1.5279,  ..., 1.3302, 1.3974, 1.0665],\n",
       "           [1.0484, 0.4942, 1.0998,  ..., 0.9541, 1.2037, 0.8252]],\n",
       "\n",
       "          [[1.2650, 0.7089, 0.9313,  ..., 0.8448, 0.8512, 0.7844],\n",
       "           [2.0801, 1.2279, 1.9333,  ..., 1.9121, 2.0365, 1.7187],\n",
       "           [2.1334, 1.6876, 2.2982,  ..., 2.0595, 1.8346, 2.0145],\n",
       "           [1.5161, 1.5621, 1.5721,  ..., 1.0958, 1.0341, 1.2319]]]]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7a3bc294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 7, 10])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(size=(4,7,10,1))\n",
    "torch.transpose(y, 2,3).transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b3b28575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usecase of a mat mul in a linear layer of NN where output = x.W + b\n",
    "linear = torch.nn.Linear(in_features=2, out_features=10)\n",
    "x = torch.rand(size=(4,2))\n",
    "output = linear(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0159c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2911fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "552352fd",
   "metadata": {},
   "source": [
    "### Aggregations on Tensors\n",
    "1. .min, .max, .mean, .sum can all be properties as well as torch functions on tensor   \n",
    "2. default versions of aggregations work by flattening the whole tensor. Alternatively aggregations can be done along a certain dimension  \n",
    "3. .argmax, .argmin to find the index of the max and min in a tensor   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8743f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(0.)\n",
      "tensor(90.) tensor(90.)\n",
      "tensor(45.) tensor(90.)\n",
      "tensor(450.) tensor(450.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,100,10).type(torch.float32)\n",
    "print(x.min(), torch.min(x))\n",
    "print(x.max(), torch.max(x))\n",
    "print(x.mean(), torch.max(x))\n",
    "print(x.sum(), torch.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "617f7719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9) tensor(9)\n",
      "tensor(0) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(x.argmax(), torch.argmax(x))\n",
    "print(x.argmin(), torch.argmin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f42c238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(5,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c8c029d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([[0.0786, 0.0361],\n",
       "        [0.0670, 0.0591],\n",
       "        [0.0762, 0.4678]]),\n",
       "indices=tensor([[2, 1],\n",
       "        [2, 3],\n",
       "        [1, 2]]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4100ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66020893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb367ed",
   "metadata": {},
   "source": [
    "### Manipulating Tensor shapes\n",
    "1. __.reshape(  )__ _(property/function)_ used to morph the tensor into a different shape  \n",
    "    * Compatibility with destination shape is product of all current dims should match product of all desired dims  \n",
    "    * A single dimension can be defined -1 in which case it will be inferred from the remaining dims  \n",
    "2. __.view(  )__ _(property)_ used to view a reshaped tensor similar to above, but the view shares the same data with the original tensor   \n",
    "3. __.stack( )__ _(function)_ used to concat a list of tensors along a certain dim, all tensors must be of same size. Dim along which to stack can be 0 to N+1 where N is ndim of tensors to be stacked    \n",
    "4. __.squeeze( )__ _(function, property)_ remove all dims with value 1 from a tensor and reduce the ndim of a tensor, if a dim is specified squeeze happens only on that dim if possible  \n",
    "5. __.unsqueeze( )__ _(function, property)_ add a new dim with value 1to a tensor and increase the ndim of a tensor by 1 at a specified dim  \n",
    "6. __.permute( )__ _(function, property)_ to reshuffle the dimensions of a tensor, different from reshaping* but only returns a view    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6f927165",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(2,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "598c8a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0949, 0.8495],\n",
       "         [0.3976, 0.3502],\n",
       "         [0.0321, 0.6779]],\n",
       "\n",
       "        [[0.9089, 0.8128],\n",
       "         [0.7530, 0.7042],\n",
       "         [0.2968, 0.8736]]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "752027f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0949, 0.8495],\n",
       "        [0.3976, 0.3502],\n",
       "        [0.0321, 0.6779],\n",
       "        [0.9089, 0.8128],\n",
       "        [0.7530, 0.7042],\n",
       "        [0.2968, 0.8736]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8812ed9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0949, 0.8495, 0.3976, 0.3502, 0.0321, 0.6779],\n",
       "        [0.9089, 0.8128, 0.7530, 0.7042, 0.2968, 0.8736]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4fb15187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0949,  0.8495],\n",
       "         [ 0.3976,  0.3502],\n",
       "         [ 0.0321, 10.0000]],\n",
       "\n",
       "        [[ 0.9089,  0.8128],\n",
       "         [ 0.7530,  0.7042],\n",
       "         [ 0.2968,  0.8736]]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(2,-1)\n",
    "y[0,5] = 10\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0b695305",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0949,  0.8495],\n",
       "          [ 0.3976,  0.3502],\n",
       "          [ 0.0321, 10.0000]],\n",
       "\n",
       "         [[ 0.9089,  0.8128],\n",
       "          [ 0.7530,  0.7042],\n",
       "          [ 0.2968,  0.8736]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0949,  0.8495],\n",
       "          [ 0.3976,  0.3502],\n",
       "          [ 0.0321, 10.0000]],\n",
       "\n",
       "         [[ 0.9089,  0.8128],\n",
       "          [ 0.7530,  0.7042],\n",
       "          [ 0.2968,  0.8736]]]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x,x], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8aabdf76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0949,  0.0949],\n",
       "          [ 0.8495,  0.8495]],\n",
       "\n",
       "         [[ 0.3976,  0.3976],\n",
       "          [ 0.3502,  0.3502]],\n",
       "\n",
       "         [[ 0.0321,  0.0321],\n",
       "          [10.0000, 10.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9089,  0.9089],\n",
       "          [ 0.8128,  0.8128]],\n",
       "\n",
       "         [[ 0.7530,  0.7530],\n",
       "          [ 0.7042,  0.7042]],\n",
       "\n",
       "         [[ 0.2968,  0.2968],\n",
       "          [ 0.8736,  0.8736]]]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([x,x], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bc89ca56",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0949],\n",
       "          [ 0.8495]],\n",
       "\n",
       "         [[ 0.3976],\n",
       "          [ 0.3502]],\n",
       "\n",
       "         [[ 0.0321],\n",
       "          [10.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9089],\n",
       "          [ 0.8128]],\n",
       "\n",
       "         [[ 0.7530],\n",
       "          [ 0.7042]],\n",
       "\n",
       "         [[ 0.2968],\n",
       "          [ 0.8736]]]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2,3,1,2).squeeze().unsqueeze(dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ee6e4d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0949,  0.8495],\n",
       "         [ 0.9089,  0.8128]],\n",
       "\n",
       "        [[ 0.3976,  0.3502],\n",
       "         [ 0.7530,  0.7042]],\n",
       "\n",
       "        [[ 0.0321, 10.0000],\n",
       "         [ 0.2968,  0.8736]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756fc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6f7226f",
   "metadata": {},
   "source": [
    "### Indexing Tensors\n",
    "1. Indexing goes from outermost dim -> inner dim (as it should logically)  \n",
    "2. Slicing and dicing works same as python arrays logic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "17f7da35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0250eba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][1][1] == x[0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fe45df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,-1] == x[:,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ba86f5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0949,  0.8495],\n",
       "         [ 0.0321, 10.0000]],\n",
       "\n",
       "        [[ 0.9089,  0.8128],\n",
       "         [ 0.2968,  0.8736]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa1d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049696dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0267faf0",
   "metadata": {},
   "source": [
    "### Determinism and Seeding Random Number Generators (RNG)\n",
    "It is important to have determinism across different runs of an algorithm, but since random numbers are involved at every part of an ML algo this needs to be explicitly dealt with. More reference here : https://pytorch.org/docs/stable/notes/randomness.html  \n",
    "1. __torch.manual_seed( )__ setting this at the beginning of the application with a known seed can cause torch to initialize the RNG for all devices to a certain defined value. At each use of the RNG the internal seed will change, so to get exact values repeatedly within a single application RNG has to be seeded before every call. In an application with fixed number of operations, the RNG once seeded will change at every use but deterministically.  \n",
    "2. __torch.use_deterministic_algorithms()__ setting this to True forces pt to use algorithms that have non-determinism in them strictly deterministically else throw an error  \n",
    "3. More sources of randomness in pt could be from using the DataLoader where every thread may spin off a different state of RNG. Real docs on how to curb this.  \n",
    "4. There can be more sources of randomness like Python has its own RNG and Numpy uses its own. Seeding these can be very important too if the script uses these.   \n",
    "5. Finally the optimization device like GPU can have low level code that has inbuilt randomness. Like CuDNN for CUDA has some inbuilt non-determinism in choosing the best algo etc. Some of these can be controlled by setting env variables etc. Docs for more.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d92c8822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14170024646278309570"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3dc54a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False, False, False],\n",
       "         [False, False, False]]),\n",
       " tensor([[True, True, True],\n",
       "         [True, True, True]]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(size=(2,3))\n",
    "y = torch.rand(size=(2,3))\n",
    "torch.manual_seed(42)\n",
    "z = torch.rand(size=(2,3))\n",
    "x==y, x==z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf45380f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e16088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9a7953",
   "metadata": {},
   "source": [
    "### Using GPU on Mac\n",
    "Mac's MPS (Metal Performance Shaders) similar to CUDA is an API to use the underlying GPU in their chips for faster performance and Torch has backend that can use those accelerations.  \n",
    "To use the GPU we have to transfer the data and compute to the GPU device explicitly. Data transferred to GPUs usually means a copy is created. Some functions may not be usable with the GPU copy - like numpy ops, which are CPU only operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ab76e3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "58045092",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_metal = x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9dc4076b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='mps', index=0), device(type='cpu'))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_metal.device, x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "754b570b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88226926, 0.91500396, 0.38286376],\n",
       "       [0.95930564, 0.3904482 , 0.60089535]], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_metal.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf294060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca4e8ca1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Create a random tensor with shape (7, 7).  \n",
    "2. Perform a matrix multiplication on the tensor from 2 with another random tensor with shape (1, 7) (hint: you may have to transpose the second tensor).  \n",
    "3. Set the random seed to 0 and do exercises 2 & 3 over again.  \n",
    "4. Speaking of random seeds, we saw how to set it with torch.manual_seed() but is there a GPU equivalent? (hint: you'll need to look into the documentation for torch.cuda for this one). If there is, set the GPU random seed to 1234.  \n",
    "5. Create two random tensors of shape (2, 3) and send them both to the GPU (you'll need access to a GPU for this). Set torch.manual_seed(1234) when creating the tensors (this doesn't have to be the GPU random seed).  \n",
    "6. Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).  \n",
    "7. Find the maximum and minimum values of the output of 7.  \n",
    "8. Find the maximum and minimum index values of the output of 7.  \n",
    "9. Make a random tensor with shape (1, 1, 1, 10) and then create a new tensor with all the 1 dimensions removed to be left with a tensor of shape (10). Set the seed to 7 when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3aa5e504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 7])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1\n",
    "t = torch.rand(size=(7,7))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e084b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.rand(size=(1,7))\n",
    "print(m.shape)\n",
    "(t.matmul(m.T)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e72883d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9822],\n",
       "        [1.8049],\n",
       "        [1.0277],\n",
       "        [1.6338],\n",
       "        [2.1819],\n",
       "        [1.8437],\n",
       "        [2.1708]])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed = 0\n",
    "t = torch.rand((7,7))\n",
    "m = torch.rand((1,7))\n",
    "t.matmul(m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b05ed4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seed for mps device is taken care of by the same torch.manual_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "59654374",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed = 1234\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "t = torch.rand((2,3))\n",
    "m = torch.rand((2,3))\n",
    "td = t.to(device)\n",
    "md = m.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "62baec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = torch.matmul(td, md.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "59a3ef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4158, device='mps:0'), tensor(1.1078, device='mps:0'))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.min(), rd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d3c2ba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1, device='mps:0'), tensor(2, device='mps:0'))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.argmin(), rd.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "50cf1762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5723, 0.3705, 0.7069, 0.3096, 0.1764, 0.8649, 0.2726, 0.3998,\n",
      "           0.0026, 0.8346]]]]) torch.Size([1, 1, 1, 10])\n",
      "tensor([0.5723, 0.3705, 0.7069, 0.3096, 0.1764, 0.8649, 0.2726, 0.3998, 0.0026,\n",
      "        0.8346]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed = 7\n",
    "t = torch.rand((1,1,1,10))\n",
    "print(t, t.shape)\n",
    "r = t.squeeze()\n",
    "print(r, r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2827d10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
